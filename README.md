# Panoramic Scene Generation using Conditional GAN
**Generating Seamless 360¬∞ Panoramic Images across Multiple Scene Categories**

## Abstract
This repository presents a Conditional Generative Adversarial Network (GAN) for generating panoramic images (128√ó256 resolution) across three distinct scene categories: city, forest, and rural. The model achieves competitive performance, with an FID of 70.04 and an Inception Score of 2.65, on a challenging dataset of 25,934 panoramic images. Key contributions include systematic hyperparameter optimization to overcome mode collapse during training, a progressive fine-tuning strategy spanning 250 epochs, and careful loss balancing through the use of spectral normalization and gradient penalties.

## Table of Contents
- [Introduction](#introduction)
- [Architecture](#architecture)
- [Dataset](#dataset)
- [Training Methodology](#training-methodology)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [Code Structure](#code-structure)

---

## Introduction
Panoramic image generation poses unique challenges:

- **Wide field of view:** 2:1 aspect ratio with diverse spatial elements
- **Multi-scale structure:** Requires capturing both foreground and background
- **Class imbalance:** Real-world datasets often have uneven class distributions

This work addresses these challenges through:

- Custom GAN architecture optimized for 2:1 aspect ratio (128√ó256)
- Progressive training strategy with careful discriminator-generator balance
- Class-conditional generation using projection discriminator
- Exponential Moving Average (EMA) for stable convergence

---

## Architecture

### Network Overview

![Panoramic GAN Architecture](doc/panoramic_gan_architecture (3).svg)

The architecture consists of two main components:

### Generator Network
**Input:** [Noise(128-dim) + Class Embedding(128-dim)] ‚Üí 256-dim

The generator uses progressive upsampling with residual connections:

1. **Fully Connected Layer:** 256 ‚Üí 512√ó8√ó16
2. **Stage 1:** ConvTranspose2d (512‚Üí256, k=4, s=2, p=1) + ResBlock(256) ‚Üí 16√ó32
3. **Stage 2:** ConvTranspose2d (256‚Üí128, k=4, s=2, p=1) + ResBlock(128) ‚Üí 32√ó64
4. **Stage 3:** ConvTranspose2d (128‚Üí64, k=4, s=2, p=1) + ResBlock(64) ‚Üí 64√ó128
5. **Stage 4:** ConvTranspose2d (64‚Üí32, k=4, s=2, p=1) + ResBlock(32) ‚Üí 128√ó256
6. **Final Layer:** Conv2d (32‚Üí3, k=7, s=1, p=3) + Tanh ‚Üí Output RGB (128√ó256√ó3)

**Key Components:**
- Residual blocks with Instance Normalization (Conv ‚Üí InstanceNorm ‚Üí ReLU) for improved gradient flow
- Progressive upsampling architecture
- Class-conditional generation via embedding

### Discriminator Network
**Input:** RGB Image (3√ó128√ó256) + Class Label

The discriminator uses spectral normalization throughout:

1. **Stage 1:** Conv2d + SN (3‚Üí64, k=4, s=2, p=1) + LeakyReLU(0.2) ‚Üí 64√ó128
2. **Stage 2:** Conv2d + SN (64‚Üí128, k=4, s=2, p=1) + InstanceNorm + LeakyReLU(0.2) ‚Üí 32√ó64
3. **Stage 3:** Conv2d + SN (128‚Üí256, k=4, s=2, p=1) + InstanceNorm + LeakyReLU(0.2) ‚Üí 16√ó32
4. **Stage 4:** Conv2d + SN (256‚Üí512, k=4, s=2, p=1) + InstanceNorm + LeakyReLU(0.2) ‚Üí 8√ó16
5. **Global Average Pooling:** 512√ó8√ó16 ‚Üí 512-dim
6. **Projection Head:** Linear(512‚Üí1) + Class Embedding(512-dim) ‚Üí Real/Fake Score

**Key Components:**
- Spectral normalization for Lipschitz constraint (applied to all Conv layers)
- Instance normalization in stages 2-4 
- Projection discriminator for class conditioning
- Multi-scale feature extraction
- R1 gradient penalty for regularization

---

## Dataset

### Structure and Statistics
```
Proprietary Panoramic Dataset
‚îú‚îÄ‚îÄ city/      8,739 images  (33.7%)
‚îú‚îÄ‚îÄ forest/    1,591 images  (6.1%)
‚îî‚îÄ‚îÄ rural/     15,604 images (60.2%)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:         25,934 panoramic images
```

**Note:** The dataset used in this work is proprietary and cannot be publicly shared due to licensing and privacy restrictions. However, the code is fully functional and can be used with your own panoramic image dataset organized in the same structure.

### Dataset Characteristics
| Property | Value |
|----------|-------|
| Total Images | 25,934 |
| Original Resolution | 832√ó416 pixels (2:1 aspect ratio) |
| Training Resolution | 128√ó256 pixels (downsampled) |
| Format | JPEG/PNG |
| Scene Types | City, Forest, Rural |

### Data Split
The dataset was split into:
- **Training Set:** 80% (20,747 images)
- **Validation Set:** 20% (5,187 images)

### Evaluation Protocol
**Evaluation was performed on the validation set:**
- Generated samples: 1,000 images per model
- Real validation samples: 1,000 randomly sampled from validation set
- No overlap between training and evaluation data
- FID and IS calculated using Inception-v3 features

### Class Distribution Analysis

**City (8,739 images - 33.7%)**
- Urban streets and buildings
- Varied conditions: blurred vehicles, blurred buildings, tunnel interiors
- Diverse lighting and weather conditions

**Forest (1,591 images - 6.1%)**
- Natural forest landscapes
- Limited diversity: majority similar forest paths
- ~25-30% contain rural elements (not pure forest)
- Smallest class - potential for class imbalance effects

**Rural (15,604 images - 60.2%)**
- Countryside and rural scenes
- Largest class - model shows strongest convergence on this category
- Dominant class influences generation distribution

### Data Preprocessing
Applied transforms during training:
```python
transforms.Compose([
    transforms.Resize((128, 256), Image.LANCZOS),  # Downscale from 416√ó832
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, 
                          saturation=0.1, hue=0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # [-1, 1]
])
```

---

## Training Methodology

### Phase 1: Initial Training (Epochs 1-110)
**Configuration:**
```python
lr_g = 1.5e-4
lr_d = 2.0e-4
batch_size = 4
r1_gamma = 12.0
ema_decay = 0.995
```

**Challenge Encountered:**
Generator losses became continuously negative after epoch 75, indicating risk of mode collapse.

**Solution:**
Reduced learning rates by 3-5√ó to stabilize training dynamics.

### Phase 2: Stability Optimization (Epochs 110-200)

**Version 1 (Epochs 110-130): Initial Stabilization**
```python
lr_g = 5e-5    # ‚Üì 3√ó reduction
lr_d = 1e-4    # ‚Üì 2√ó reduction
ema_decay_later = 0.9995
```

**Version 2 (Epochs 130-170): Detail Enhancement**
```python
lr_g = 2e-5
lr_d = 5e-5
r1_gamma = 12.0
ema_decay_later = 0.9998
```

**Version 3 (Epochs 170-200): Final Fine-tuning**
```python
lr_g = 1e-5
lr_d = 2e-5
r1_gamma = 15.0
ema_decay_later = 0.9999
```

**Training Losses at Epoch 200:**
- Discriminator Loss: 1.9088 (healthy range)
- Generator Loss: 0.4778 (stable convergence)

### Phase 3: Extended Training (Epochs 200-250)
**Configuration:**
```python
lr_g = 5e-6    # Ultra-conservative
lr_d = 1e-5
r1_gamma = 20.0
```

**Outcome:**
Marginal improvements observed. Checkpoint_200 selected as optimal based on evaluation metrics.

### Loss Functions

**Adversarial Loss (Hinge Loss):**
```python
L_D = ùîº[max(0, 1 - D(x_real))] + ùîº[max(0, 1 + D(G(z)))]
L_G = -ùîº[D(G(z))]
```

**R1 Gradient Penalty:**
```python
L_R1 = (Œ≥/2) ¬∑ ùîº[||‚àáD(x_real)||¬≤]
```

**Total Discriminator Loss:**
```python
L_D_total = L_D + Œª_R1 ¬∑ L_R1
```

### Training Infrastructure
| Component | Specification |
|-----------|--------------|
| Platform | Kaggle Notebooks |
| GPU | NVIDIA Tesla P100 (16GB VRAM) |
| Framework | PyTorch 2.6.0 |
| CUDA | 12.4 |
| cuDNN | 9.3.0 |
| Python | 3.11.13 |
| Batch Size | 2-4 (GPU memory constrained) |
| Optimizer | Adam (Œ≤‚ÇÅ=0.0, Œ≤‚ÇÇ=0.9) |

---

## Results

### Quantitative Evaluation
| Model | FID ‚Üì | Inception Score ‚Üë | Training Epochs |
|-------|-------|-------------------|-----------------|
| EMA_Generator_200 | **70.04** | **2.652 ¬± 0.139** | 200 |
| Checkpoint_200 | 70.60 | 2.602 ¬± 0.085 | 200 |
| EMA_Generator_250 | 71.04 | 2.696 ¬± 0.147 | 250 |
| Checkpoint_250 | 72.28 | 2.607 ¬± 0.125 | 250 |

**Evaluation Protocol:**
- Generated samples: 1,000 images per model
- Real validation samples: 1,000 randomly sampled from validation set
- FID: Inception-v3 features, Fr√©chet distance (Heusel et al., NIPS 2017)
- IS: 10 splits for statistical robustness (Salimans et al., NIPS 2016)

### Best Model Analysis

**EMA_Generator_200 (Recommended)**

**Strengths:**
- ‚úÖ Lowest FID score (70.04) - best distribution match
- ‚úÖ Competitive Inception Score (2.65) - good quality/diversity
- ‚úÖ Optimal training convergence at 200 epochs
- ‚úÖ EMA smoothing provides stable outputs

**Performance Metrics:**
```
FID Score: 70.04  (Lower is better)
  ‚Üí Measures distribution similarity with validation images
  ‚Üí Competitive for panoramic generation

Inception Score: 2.652 ¬± 0.139  (Higher is better)
  ‚Üí Evaluates object recognizability and diversity
  ‚Üí Confidence interval: [2.513, 2.791]
```

### Class-Specific Performance
Due to imbalanced dataset (Rural: 60%, City: 34%, Forest: 6%):

**Expected Behavior:**
- Model shows strongest convergence on rural scenes (largest class)
- City scenes: Good diversity despite varied conditions
- Forest scenes: Limited by small dataset size (1,591 images)

---

## Installation

### Requirements

**Recommended Hardware:**
- GPU: NVIDIA GPU with 12GB+ VRAM (GTX 1080 Ti / RTX 3060 or better)
- RAM: 16GB+ system memory

**Software Used:**
- Python 3.11.13
- CUDA 12.4
- PyTorch 2.6.0

### Setup Instructions

#### 1. Clone Repository
```bash
git clone https://github.com/yourusername/panoramic-gan.git
cd panoramic-gan
```

#### 2. Create Virtual Environment
```bash
# Create virtual environment
python -m venv venv

# Activate (Linux/Mac)
source venv/bin/activate

# Activate (Windows)
venv\Scripts\activate
```

#### 3. Install PyTorch (GPU)
For CUDA 12.4:
```bash
pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 \
  --index-url https://download.pytorch.org/whl/cu124
```

#### 4. Install Other Dependencies
```bash
pip install -r requirements.txt
```

---

## Usage

### Preparing Your Own Dataset

Since the original dataset is not publicly available, you'll need to prepare your own panoramic image dataset. Organize images in this structure:

```
your_dataset/
‚îú‚îÄ‚îÄ city/
‚îÇ   ‚îú‚îÄ‚îÄ image001.jpg
‚îÇ   ‚îú‚îÄ‚îÄ image002.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ forest/
‚îÇ   ‚îú‚îÄ‚îÄ image001.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ rural/
    ‚îú‚îÄ‚îÄ image001.jpg
    ‚îî‚îÄ‚îÄ ...
```

**Requirements for your dataset:**
- Images should have a 2:1 aspect ratio (e.g., 832√ó416, 1024√ó512, etc.)
- Minimum recommended: 1000+ images per class
- Organize into class-specific folders
- Supported formats: JPEG, PNG

**Data Split Recommendation:**
- 80% training, 20% validation split
- Sample 1,000 images from validation for FID/IS evaluation

### Training

#### Split Dataset (Required - First Time Only)

Before training, you must split your dataset into train/validation sets:
```bash
python train.py \
  --root /path/to/your_dataset \
  --split_dataset \
  --train_ratio 0.8
```

**What this does:**
- Creates `split_info.json` in your dataset directory
- Splits data into 80% training / 20% validation
- Records which specific images belong to train vs validation

**You only run this once.** All subsequent training and evaluation will use this split.

#### Basic Training
```bash
python train.py \
  --root /path/to/your_dataset \
  --out ./outputs \
  --epochs 200 \
  --batch_size 4 \
  --lr_g 1e-5 \
  --lr_d 2e-5
```

#### Monitor Training
Samples are saved every 500 batches to `outputs/`:
- `panorama_epoch_XXX.png` - Per-epoch samples
- `sample_epochX_batchY.png` - During-training samples

### Advanced Training Options

**For Limited GPU Memory (8GB):**
```bash
python train.py \
  --root /path/to/dataset \
  --batch_size 2 \
  --epochs 200
```

**For Faster Convergence (Powerful GPU):**
```bash
python train.py \
  --root /path/to/dataset \
  --batch_size 6 \
  --lr_g 2e-5 \
  --lr_d 4e-5 \
  --epochs 200
```

**Progressive Training (Recommended):**
```bash
# Phase 1: Initial training (epochs 1-110)
python train.py --root /path/to/dataset --epochs 110 --lr_g 1.5e-4 --lr_d 2e-4

# Phase 2: Stability (epochs 110-200)
python train.py --resume ./outputs/checkpoints/checkpoint_epoch_110.pt --epochs 200 --lr_g 1e-5 --lr_d 2e-5

# Phase 3: Fine-tuning (epochs 200-250)
python train.py --resume ./outputs/checkpoints/checkpoint_epoch_200.pt --epochs 250 --lr_g 5e-6 --lr_d 1e-5
```

### Evaluation
```bash
python evaluate.py \
  --model_dir ./outputs/checkpoints \
  --dataset_path /path/to/dataset \
  --output_dir ./evaluation_results \
  --num_samples 1000 \
  --use_validation
```

**What it does:**
- Automatically discovers all `.pt` and `.pth` model files in `--model_dir`
- Evaluates each model found
- Generates FID and Inception Score for each
- Saves comparative results

**Outputs:**
- `detailed_results_TIMESTAMP.json` - Full metrics
- `summary_results_TIMESTAMP.csv` - Summary table
- `MODEL_NAME_samples.png` - Generated samples

### Image Generation
```bash
python generate.py \
  --model outputs/final_generator_ema.pth \
  --num_images 25 \
  --class_type city \
  --output_dir ./generated
```

**Model File Types:**
- **`.pth` files** (recommended): Direct model weights
  - `final_generator_ema.pth` - EMA-smoothed generator (best quality)
  - `final_generator.pth` - Regular generator
  
- **`.pt` files**: Full training checkpoints
  - `checkpoint_epoch_200.pt` - Contains generator, discriminator, optimizers
  - Script automatically loads EMA generator if available, otherwise regular generator

**Class Options:**
- `city` - Urban streets and buildings
- `forest` - Natural forest landscapes  
- `rural` - Countryside and rural scenes

**Output:**
- Grid image: `./generated/city_generated.png` (25 images in 5√ó5 layout)

---

## Code Structure

### File Organization
```
panoramic-gan/
‚îÇ
‚îú‚îÄ‚îÄ train.py                    # Main training script
‚îú‚îÄ‚îÄ models.py                   # GAN architecture (Generator, Discriminator, EMA)
‚îú‚îÄ‚îÄ evaluate.py                 # Evaluation with FID/IS metrics
‚îú‚îÄ‚îÄ generate.py                 # Image generation inference
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îÇ
‚îú‚îÄ‚îÄ docs/                       # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ architecture.png        # Architecture diagram
‚îÇ
‚îî‚îÄ‚îÄ samples/                    # Generated samples
```

### Code Breakdown

**models.py (Network Architectures)**

Contains:
- `ImprovedGenerator` - Generator network class
- `ImprovedDiscriminator` - Discriminator network class
- `ResidualBlock` - Residual connection building block
- `EMA` - Exponential Moving Average helper class
- `init_weights()` - Weight initialization function

Key Classes:
```python
class ImprovedGenerator(nn.Module):
    def __init__(self, noise_dim, class_dim, img_channels)
    def forward(self, noise, labels)

class ImprovedDiscriminator(nn.Module):
    def __init__(self, class_dim, img_channels)
    def forward(self, x, labels)

class ResidualBlock(nn.Module):
    def __init__(self, channels, use_spectral_norm=False)

class EMA:
    def __init__(self, model, decay)
    def update(self, model)
```

**train.py (Main Training Loop)**

Contains:
- Argument parsing and configuration
- Dataset loading (`PanoramicDataset` class)
- Model instantiation (Generator, Discriminator, EMA)
- Training loop with loss calculation
- Checkpoint saving and sample generation
- Progressive learning rate scheduling

Key Classes/Functions:
```python
class PanoramicDataset(Dataset)      # Custom dataset loader
def init_weights(m)                  # Weight initialization
def save_samples(gen_model, epoch)   # Generate samples
# Main training loop (epochs 1-250)
```

**evaluate.py (Evaluation Metrics)**

Contains:
- `ValidatedGANMetrics` - Combined FID and IS calculator
- `DatasetLoader` - Loads real images with train/val split support
- `AcademicPanoramicEvaluator` - Main evaluation orchestrator
- Feature extraction using Inception-v3
- Statistical validation and confidence intervals

Key Classes:
```python
class ValidatedGANMetrics:
    def calculate_fid(self, real_images, fake_images)
    def calculate_inception_score(self, images)
    def _extract_features(self, images)
    def _get_predictions(self, images)
    
class DatasetLoader:
    def load_real_images(self, max_images, use_validation)

class AcademicPanoramicEvaluator:
    def evaluate_model(self, model_path, model_name, real_images)
```

**generate.py (Inference Script)**

Contains:
- Model loading from checkpoint
- Batch generation with specified class
- Image post-processing and saving
- Grid visualization

Key Functions:
```python
def load_generator(checkpoint_path)           # Load trained model
def generate_images(generator, num, class_idx) # Generate batch
def save_image_grid(images, output_path)      # Save results
```

---
